% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
% \usepackage{extsizes}
\documentclass[8pt]{extarticle}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}

\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\date{}

\usepackage{minted}
\usepackage{tcolorbox}
\tcbuselibrary{minted,skins}

\newtcblisting{codebox}[2]{
  listing engine=minted,
  colback=codeboxbg,
  colframe=black!70,
  listing only,
  minted style=colorful,
  minted language=#1,
  minted options={linenos=true,numbersep=3mm,texcl=true},
  title=#2,
  left=5mm,enhanced,
  overlay={\begin{tcbclipinterior}\fill[black!25] (frame.south west)
            rectangle ([xshift=5mm]frame.north west);\end{tcbclipinterior}}
}
\definecolor{codeboxbg}{rgb}{0.85,0.85,0.85}

\usepackage{multicol}
\usepackage[landscape]{geometry}
\geometry{top=.4in,left=.2in,right=.2in,bottom=.4in}

\title{Pytorch Cheat Sheet}
\author{Shivanshu Gupta}
\date{}
% ref: https://tex.stackexchange.com/a/623
\usepackage{titling}
\posttitle{\par\end{center}}
\setlength{\droptitle}{-10ex}

\begin{document}
\begin{multicols}{2}
\maketitle
\vspace{-12ex}

\hypertarget{imports}{%
\section{Imports}\label{imports}}

\begin{codebox}{python}{\hypertarget{general}{%
General\label{general}}}
import torch                                        # root package
from torch.utils.data import Dataset, Dataloader    # dataset representation and 
                                                    # loading
\end{codebox}

\begin{codebox}{python}{\hypertarget{neural-network-api}{%
Neural Network API\label{neural-network-api}}}
import torch.autograd as autograd     # computation graph
from torch import Tensor              # tensor node in the computation graph
import torch.nn as nn                 # neural networks
import torch.nn.functional as F       # layers, activations and more
import torch.optim as optim           # optimizers e.g. SGD, ADAM, etc.
from torch.jit import script, trace   # hybrid frontend decorator and tracing jit
\end{codebox}

See \href{https://pytorch.org/docs/stable/autograd.html}{autograd},
\href{https://pytorch.org/docs/stable/nn.html}{nn},
\href{https://pytorch.org/docs/stable/nn.html\#torch-nn-functional}{functional}
and \href{https://pytorch.org/docs/stable/optim.html}{optim}

\begin{codebox}{python}{\hypertarget{torchscript-and-jit}{%
Torchscript and JIT\label{torchscript-and-jit}}}
torch.jit.trace()   # takes your module or function and an example
                    # data input, and traces the computational steps
                    # that the data encounters as it progresses through the model
@script             # decorator used to indicate data-dependent
                    # control flow within the code being traced
\end{codebox}

See \href{https://pytorch.org/docs/stable/jit.html}{Torchscript}

\begin{codebox}{python}{\hypertarget{onnx}{%
ONNX\label{onnx}}}
torch.onnx.export(model, dummy data, xxxx.proto)    # exports an ONNX formatted 
                                                    # model using a trained model, 
                                                    # dummy data and the desired 
                                                    # file name
model = onnx.load("alexnet.proto")                  # load an ONNX model
onnx.checker.check_model(model)                     # check that the model
                                                    # IR is well formed
onnx.helper.printable_graph(model.graph)            # print a human readable
                                                    # representation of the graph
\end{codebox}

See \href{https://pytorch.org/docs/stable/onnx.html}{onnx}

\begin{codebox}{python}{\hypertarget{vision}{%
Vision\label{vision}}}
from torchvision import datasets, models, transforms    # vision datasets,
                                                        # architectures &
                                                        # transforms
import torchvision.transforms as transforms             # composable transforms
\end{codebox}

See
\href{https://pytorch.org/docs/stable/torchvision/index.html}{torchvision}

\begin{codebox}{python}{\hypertarget{distributed-training}{%
Distributed Training\label{distributed-training}}}
import torch.distributed as dist          # distributed communication
from multiprocessing import Process       # memory sharing processes
\end{codebox}

See \href{https://pytorch.org/docs/stable/distributed.html}{distributed}
and
\href{https://pytorch.org/docs/stable/multiprocessing.html}{multiprocessing}

\hypertarget{tensors}{%
\section{Tensors}\label{tensors}}

\begin{codebox}{python}{\hypertarget{creation}{%
Creation\label{creation}}}
torch.randn(*size)          # tensor with independent N(0,1) entries
torch.[ones|zeros](*size)   # tensor with all 1's [or 0's]
torch.Tensor(L)             # create tensor from [nested] list or ndarray L
x.clone()                   # clone of x
with torch.no_grad():       # code wrap that stops autograd from tracking 
                            # tensor history
requires_grad=True          # arg, when set to True, tracks computation
                            # history for future derivative calculations
\end{codebox}

See \href{https://pytorch.org/docs/stable/tensors.html}{tensor}

\begin{codebox}{python}{\hypertarget{dimensionality}{%
Dimensionality\label{dimensionality}}}
x.size()                        # return tuple-like object of dimensions
torch.cat(tensor_seq, dim=0)    # concatenates tensors along dim
x.view(a,b,...)                 # reshapes x into size (a,b,...)
x.view(-1,a)                    # reshapes x into size (b,a) for some b
x.transpose(a,b)                # swaps dimensions a and b
x.permute(*dims)                # permutes dimensions
x.unsqueeze(dim)                # tensor with added axis
x.unsqueeze(dim=2)              # (a,b,c) tensor -> (a,b,1,c) tensor
\end{codebox}

See \href{https://pytorch.org/docs/stable/tensors.html}{tensor}

\begin{codebox}{python}{\hypertarget{algebra}{%
Algebra\label{algebra}}}
A.mm(B)     # matrix multiplication
A.mv(x)     # matrix-vector multiplication
x.t()       # matrix transpose
\end{codebox}

See
\href{https://pytorch.org/docs/stable/torch.html?highlight=mm\#math-operations}{math
operations}

\begin{codebox}{python}{\hypertarget{gpu-usage}{%
GPU Usage\label{gpu-usage}}}
torch.cuda.is_available                 # check for cuda
x.cuda()                                # move x's data from
                                        # CPU to GPU and return new object

x.cpu()                                 # move x's data from GPU to CPU
                                        # and return new object

if not args.disable_cuda \
    and torch.cuda.is_available():      # device agnostic code
    args.device = torch.device('cuda')  # and modularity
else:                                   #
    args.device = torch.device('cpu')   #

net.to(device)                          # recursively convert their
                                        # parameters and buffers to
                                        # device specific tensors

mytensor.to(device)                     # copy your tensors to a device
                                        # (gpu, cpu)
\end{codebox}

See \href{https://pytorch.org/docs/stable/cuda.html}{cuda}

\hypertarget{deep-learning}{%
\section{Deep Learning}\label{deep-learning}}

\begin{codebox}{python}{\hypertarget{layers}{%
Layers\label{layers}}}
nn.Linear(m,n)                              # fully connected layer from
                                            # m to n units

nn.ConvXd(m,n,s)                            # X dimensional conv layer from
                                            # m to n channels where X???{1,2,3}
                                            # and the kernel size is s

nn.MaxPoolXd(s)                             # X dimension pooling layer
                                            # (notation as above)

nn.BatchNorm                                # batch norm layer
nn.RNN/LSTM/GRU                             # recurrent layers
nn.Dropout(p=0.5, inplace=False)            # dropout layer for any dimensional 
                                            # input
nn.Dropout2d(p=0.5, inplace=False)          # 2-dimensional channel-wise dropout
nn.Embedding(num_embeddings, embedding_dim) # (tensor-wise) mapping from
                                            # indices to embedding vectors
\end{codebox}

See \href{https://pytorch.org/docs/stable/nn.html}{nn}

\begin{codebox}{python}{\hypertarget{loss-functions}{%
Loss Functions\label{loss-functions}}}
nn.X                        # where X is BCELoss, CrossEntropyLoss,
                            # L1Loss, MSELoss, NLLLoss, SoftMarginLoss,
                            # MultiLabelSoftMarginLoss, CosineEmbeddingLoss,
                            # KLDivLoss, MarginRankingLoss, HingeEmbeddingLoss
                            # or CosineEmbeddingLoss
\end{codebox}

See \href{https://pytorch.org/docs/stable/nn.html\#loss-functions}{loss
functions}

\begin{codebox}{python}{\hypertarget{activation-functions}{%
Activation Functions\label{activation-functions}}}
nn.X                        # where X is ReLU, ReLU6, ELU, SELU, PReLU, LeakyReLU,
                            # Threshold, HardTanh, Sigmoid, Tanh,
                            # LogSigmoid, Softplus, SoftShrink,
                            # Softsign, TanhShrink, Softmin, Softmax,
                            # Softmax2d or LogSoftmax
\end{codebox}

See
\href{https://pytorch.org/docs/stable/nn.html\#non-linear-activations-weighted-sum-nonlinearity}{activation
functions}

\begin{codebox}{python}{\hypertarget{optimizers}{%
Optimizers\label{optimizers}}}
opt = optim.x(model.parameters(), ...)  # create optimizer
opt.step()                              # update weights
optim.X                                 # where X is SGD, Adadelta, Adagrad, Adam,
                                        # SparseAdam, Adamax, ASGD,
                                        # LBFGS, RMSProp or Rprop
\end{codebox}

See \href{https://pytorch.org/docs/stable/optim.html}{optimizers}

\begin{codebox}{python}{\hypertarget{learning-rate-scheduling}{%
Learning rate scheduling\label{learning-rate-scheduling}}}
scheduler = optim.X(optimizer,...)  # create lr scheduler
scheduler.step()                    # update lr at start of epoch
optim.lr_scheduler.X                # where X is LambdaLR, StepLR, MultiStepLR,
                                    # ExponentialLR or ReduceLROnPLateau
\end{codebox}

See
\href{https://pytorch.org/docs/stable/optim.html\#how-to-adjust-learning-rate}{learning
rate scheduler}

\hypertarget{data-utilities}{%
\section{Data Utilities}\label{data-utilities}}

\begin{codebox}{python}{\hypertarget{datasets}{%
Datasets\label{datasets}}}
Dataset                    # abstract class representing dataset
TensorDataset              # labelled dataset in the form of tensors
Concat Dataset             # concatenation of Datasets
\end{codebox}

See
\href{https://pytorch.org/docs/stable/data.html?highlight=dataset\#torch.utils.data.Dataset}{datasets}

\begin{codebox}{python}{\hypertarget{dataloaders-and-datasamplers}{%
Dataloaders and DataSamplers\label{dataloaders-and-datasamplers}}}
DataLoader(dataset, batch_size=1, ...)  # loads data batches agnostic
                                        # of structure of individual data points

sampler.Sampler(dataset,...)            # abstract class dealing with
                                        # ways to sample from dataset

sampler.XSampler where ...              # Sequential, Random, Subset,
                                        # WeightedRandom or Distributed
\end{codebox}

See
\href{https://pytorch.org/docs/stable/data.html?highlight=dataloader\#torch.utils.data.DataLoader}{dataloader}

\end{multicols}
\end{document}
